{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d2c0eed",
   "metadata": {},
   "source": [
    "# Implementation of the TD3 algorithm\n",
    "\n",
    "\n",
    "Reference Paper:\n",
    "[1] Fujimoto, S., van Hoof, H., & Meger, D. (2018). Addressing function approximation error in actor-critic methods. arXiv. https://arxiv.org/abs/1802.09477\n",
    "\n",
    "This notebook only contains the training code, the architectures and other utils are found in the `src` dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb76066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "\n",
    "from td3.model.td3 import TD3\n",
    "from td3.utils.metrics import RollingAverage\n",
    "from td3.utils.replay import ReplayBuffer\n",
    "\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4320e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(\n",
    "    env: gym.Env,\n",
    "    agent: TD3,  \n",
    "    runs: int = 10, \n",
    "    device: str = 'cpu'\n",
    ") -> List[float]:\n",
    "    rewards = []\n",
    "    for _ in range(runs):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0 \n",
    "        \n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                obs_torch = torch.as_tensor(obs).view(1, -1).to(device)\n",
    "                action = agent.actor(obs_torch).view(-1).cpu().numpy()\n",
    "                \n",
    "                obs_prime, reward, terminated, truncated, _ = env.step(action)\n",
    "                ep_reward += reward\n",
    "                \n",
    "                obs = obs_prime\n",
    "                done = terminated or truncated\n",
    "\n",
    "        rewards.append(ep_reward)\n",
    "        \n",
    "    return rewards\n",
    "\n",
    "\n",
    "def train(\n",
    "    env: gym.Env, \n",
    "    agent: TD3, \n",
    "    timesteps: int = 1000000, \n",
    "    val_freq: int = 5000, \n",
    "    batch_size: int = 128, \n",
    "    buffer_size: int = 150000,\n",
    "    preload: int = 1000, \n",
    "    window: int = 20, \n",
    "    num_val_runs: int = 10 \n",
    ") -> RollingAverage:\n",
    "    \n",
    "    obs_space = np.prod(env.observation_space.shape)\n",
    "    action_space = np.prod(env.action_space.shape)\n",
    "    replay = ReplayBuffer(obs_space, action_space, buffer_size)\n",
    "    \n",
    "    metrics = RollingAverage(window)\n",
    "    \n",
    "    env_test = deepcopy(env)\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    for _ in tqdm(range(preload)):\n",
    "        action = env.action_space.sample()\n",
    "        obs_prime, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        replay.update(obs, action, reward, obs_prime, done)\n",
    "        \n",
    "        obs = obs_prime\n",
    "        if done: \n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    for step in range(1, timesteps+1):\n",
    "        action = agent.explore_action(obs)\n",
    "        obs_prime, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        replay.update(obs, action, reward, obs_prime, done)\n",
    "        \n",
    "        obs = obs_prime\n",
    "        if done:\n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "        \n",
    "        # update step \n",
    "        batch_obs, batch_actions, batch_rewards, batch_next_obs, batch_dones = replay.sample(batch_size)\n",
    "        agent.update(\n",
    "            batch_obs, \n",
    "            batch_actions, \n",
    "            batch_rewards, \n",
    "            batch_next_obs, \n",
    "            batch_dones\n",
    "        )\n",
    "        \n",
    "        if step % val_freq == 0 or step == 1:\n",
    "            val_rewards = validation_step(env_test, agent, num_val_runs)\n",
    "            metrics.update(val_rewards)\n",
    "        \n",
    "        avg_reward = float(np.mean(val_rewards))\n",
    "        print(f'Timestep: {step} | Average Val Reward: {avg_reward:.4f} | Agent Timetep: {agent.timesteps}', end='\\r')\n",
    "\n",
    "    env.close()\n",
    "    env_test.close()\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c1efe9",
   "metadata": {},
   "source": [
    "Set up the environment and the agent. First let's test on a simpler environment such as Pendulum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ddab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 7423.19it/s]"
     ]
    }
   ],
   "source": [
    "env_pend = gym.make('Pendulum-v1')\n",
    "\n",
    "obs_space = np.prod(env_pend.observation_space.shape)\n",
    "action_space = np.prod(env_pend.action_space.shape)\n",
    "pend_agent = TD3(\n",
    "    obs_space, \n",
    "    action_space, \n",
    "    env_pend.action_space.high[0]\n",
    ")\n",
    "\n",
    "metrics_pend = train(\n",
    "    env_pend, \n",
    "    pend_agent, \n",
    "    timesteps=20000, \n",
    "    val_freq=2000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
